{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f650eacd-9df9-445d-8402-a8193fb4cc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import tkinter as tk\n",
    "from tkinter import Text, Label, Button, messagebox\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tkinter import scrolledtext\n",
    "from tqdm import tqdm\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37ccb054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "# Get the number of available processors\n",
    "num_processors = os.cpu_count()\n",
    "print(num_processors)\n",
    "#12 processors\n",
    "n_jobs = max(1, num_processors - 5)\n",
    "print(n_jobs)\n",
    "#7 processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b80e79b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c61fbeda-60aa-4663-9952-617075df584b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    10158\n",
      "1     8679\n",
      "Name: label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gastos ni sa political ads halos na inaasahan na ni na may mga taong</td>\n",
       "      <td>0</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tang ina tuwid na daan daw eh sya nga di straight</td>\n",
       "      <td>1</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>salamat sa walang sawang suporta ng mga taga makati ang pagbabalik in makati</td>\n",
       "      <td>0</td>\n",
       "      <td>Filipino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>putangina mo takbo pa</td>\n",
       "      <td>1</td>\n",
       "      <td>Filipino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>with selective amnesia forgetting about the past six years he spent preparing to be</td>\n",
       "      <td>0</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18832</th>\n",
       "      <td>if hoe is talking to your man she is not the problem he is the fact that hoes feel welcomed reflects on how he acts behi</td>\n",
       "      <td>1</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18833</th>\n",
       "      <td>sounds like bad bitch</td>\n",
       "      <td>1</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18834</th>\n",
       "      <td>is that all pussy bitch</td>\n",
       "      <td>1</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18835</th>\n",
       "      <td>seen scooby hoe some niggas at the gardens too</td>\n",
       "      <td>1</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18836</th>\n",
       "      <td>this bitch just blew my high</td>\n",
       "      <td>1</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18837 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                           text  \\\n",
       "0                                                          gastos ni sa political ads halos na inaasahan na ni na may mga taong   \n",
       "1                                                                             tang ina tuwid na daan daw eh sya nga di straight   \n",
       "2                                                  salamat sa walang sawang suporta ng mga taga makati ang pagbabalik in makati   \n",
       "3                                                                                                         putangina mo takbo pa   \n",
       "4                                           with selective amnesia forgetting about the past six years he spent preparing to be   \n",
       "...                                                                                                                         ...   \n",
       "18832  if hoe is talking to your man she is not the problem he is the fact that hoes feel welcomed reflects on how he acts behi   \n",
       "18833                                                                                                     sounds like bad bitch   \n",
       "18834                                                                                                   is that all pussy bitch   \n",
       "18835                                                                            seen scooby hoe some niggas at the gardens too   \n",
       "18836                                                                                              this bitch just blew my high   \n",
       "\n",
       "       label  language  \n",
       "0          0   English  \n",
       "1          1   English  \n",
       "2          0  Filipino  \n",
       "3          1  Filipino  \n",
       "4          0   English  \n",
       "...      ...       ...  \n",
       "18832      1   English  \n",
       "18833      1   English  \n",
       "18834      1   English  \n",
       "18835      1   English  \n",
       "18836      1   English  \n",
       "\n",
       "[18837 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data = pd.read_csv(\"blending/preprocessedFIL.csv\", delimiter=',')\n",
    "#data = pd.read_csv(\"Combined Total - Preprocessed (Labeled).csv\", delimiter=',' )\n",
    "data = pd.read_csv(\"Combined Total - Preprocessed (Labeled).csv\", delimiter=',' )\n",
    "label_counts = data['label'].value_counts()\n",
    "print(label_counts)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc12b983-1cb8-4f22-ade0-577a307a0e8e",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "958f0a60-9ea8-4cc8-9abf-e72e18cd88a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aahon sa</th>\n",
       "      <th>aba eh</th>\n",
       "      <th>abandoned my</th>\n",
       "      <th>abias cbn</th>\n",
       "      <th>ability to</th>\n",
       "      <th>able to</th>\n",
       "      <th>abnormal says</th>\n",
       "      <th>about and</th>\n",
       "      <th>about being</th>\n",
       "      <th>about corruption</th>\n",
       "      <th>...</th>\n",
       "      <th>yung tanong</th>\n",
       "      <th>yung tipong</th>\n",
       "      <th>yung tono</th>\n",
       "      <th>yung totoo</th>\n",
       "      <th>yung tv</th>\n",
       "      <th>yung tvc</th>\n",
       "      <th>yung vote</th>\n",
       "      <th>yung votes</th>\n",
       "      <th>yung walang</th>\n",
       "      <th>zero vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18832</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18833</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18834</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18835</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18836</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18837 rows × 12802 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       aahon sa  aba eh  abandoned my  abias cbn  ability to  able to  \\\n",
       "0           0.0     0.0           0.0        0.0         0.0      0.0   \n",
       "1           0.0     0.0           0.0        0.0         0.0      0.0   \n",
       "2           0.0     0.0           0.0        0.0         0.0      0.0   \n",
       "3           0.0     0.0           0.0        0.0         0.0      0.0   \n",
       "4           0.0     0.0           0.0        0.0         0.0      0.0   \n",
       "...         ...     ...           ...        ...         ...      ...   \n",
       "18832       0.0     0.0           0.0        0.0         0.0      0.0   \n",
       "18833       0.0     0.0           0.0        0.0         0.0      0.0   \n",
       "18834       0.0     0.0           0.0        0.0         0.0      0.0   \n",
       "18835       0.0     0.0           0.0        0.0         0.0      0.0   \n",
       "18836       0.0     0.0           0.0        0.0         0.0      0.0   \n",
       "\n",
       "       abnormal says  about and  about being  about corruption  ...  \\\n",
       "0                0.0        0.0          0.0               0.0  ...   \n",
       "1                0.0        0.0          0.0               0.0  ...   \n",
       "2                0.0        0.0          0.0               0.0  ...   \n",
       "3                0.0        0.0          0.0               0.0  ...   \n",
       "4                0.0        0.0          0.0               0.0  ...   \n",
       "...              ...        ...          ...               ...  ...   \n",
       "18832            0.0        0.0          0.0               0.0  ...   \n",
       "18833            0.0        0.0          0.0               0.0  ...   \n",
       "18834            0.0        0.0          0.0               0.0  ...   \n",
       "18835            0.0        0.0          0.0               0.0  ...   \n",
       "18836            0.0        0.0          0.0               0.0  ...   \n",
       "\n",
       "       yung tanong  yung tipong  yung tono  yung totoo  yung tv  yung tvc  \\\n",
       "0              0.0          0.0        0.0         0.0      0.0       0.0   \n",
       "1              0.0          0.0        0.0         0.0      0.0       0.0   \n",
       "2              0.0          0.0        0.0         0.0      0.0       0.0   \n",
       "3              0.0          0.0        0.0         0.0      0.0       0.0   \n",
       "4              0.0          0.0        0.0         0.0      0.0       0.0   \n",
       "...            ...          ...        ...         ...      ...       ...   \n",
       "18832          0.0          0.0        0.0         0.0      0.0       0.0   \n",
       "18833          0.0          0.0        0.0         0.0      0.0       0.0   \n",
       "18834          0.0          0.0        0.0         0.0      0.0       0.0   \n",
       "18835          0.0          0.0        0.0         0.0      0.0       0.0   \n",
       "18836          0.0          0.0        0.0         0.0      0.0       0.0   \n",
       "\n",
       "       yung vote  yung votes  yung walang  zero vote  \n",
       "0            0.0         0.0          0.0        0.0  \n",
       "1            0.0         0.0          0.0        0.0  \n",
       "2            0.0         0.0          0.0        0.0  \n",
       "3            0.0         0.0          0.0        0.0  \n",
       "4            0.0         0.0          0.0        0.0  \n",
       "...          ...         ...          ...        ...  \n",
       "18832        0.0         0.0          0.0        0.0  \n",
       "18833        0.0         0.0          0.0        0.0  \n",
       "18834        0.0         0.0          0.0        0.0  \n",
       "18835        0.0         0.0          0.0        0.0  \n",
       "18836        0.0         0.0          0.0        0.0  \n",
       "\n",
       "[18837 rows x 12802 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_df_value = 3 # set to 2: sets the minimum frequency required for a term to be considered\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=min_df_value, ngram_range=(2,2)) # 2,2= Bigram\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(data['text'])\n",
    "tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names())\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4afe8210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum the TF-IDF scores for each term across all documents\n",
    "#tfidf_scores = X_tfidf.sum(axis=0)\n",
    "\n",
    "# Get the feature names (terms) from the TF-IDF vectorizer\n",
    "#feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# Create a DataFrame to store the TF-IDF scores with their corresponding terms\n",
    "#top_tfidf_df = pd.DataFrame(tfidf_scores, columns=feature_names).transpose()\n",
    "\n",
    "# Sort the DataFrame by TF-IDF scores in descending order\n",
    "#top_tfidf_df = top_tfidf_df.sort_values(by=0, ascending=False)\n",
    "\n",
    "# Print the top TF-IDF values\n",
    "#print(\"Top TF-IDF values:\")\n",
    "#print(top_tfidf_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fe74442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top TF-IDF values for label 0:\n",
      "it is      60.745011\n",
      "of the     57.418685\n",
      "do not     54.263971\n",
      "in the     52.073185\n",
      "for the    45.798955\n",
      "is not     40.815781\n",
      "he is      37.617563\n",
      "on the     36.853737\n",
      "para sa    34.616097\n",
      "to be      34.220842\n",
      "dtype: float64\n",
      "\n",
      "Top TF-IDF values for label 1:\n",
      "commercial ni    71.642393\n",
      "do not           65.108259\n",
      "you are          55.453136\n",
      "ad ni            46.867771\n",
      "na si            39.712426\n",
      "it is            38.706878\n",
      "ng mga           38.459045\n",
      "na lang          38.097734\n",
      "sa mga           36.184096\n",
      "dtype: float64\n",
      "\n",
      "Top TF-IDF values for all labels:\n",
      "do not           119.372230\n",
      "it is             99.451889\n",
      "of the            87.134114\n",
      "commercial ni     83.910431\n",
      "in the            81.880683\n",
      "you are           80.735944\n",
      "sa mga            69.006290\n",
      "he is             67.514970\n",
      "is not            65.969073\n",
      "na si             65.098809\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Concatenate TF-IDF DataFrame with the label column\n",
    "tfidf_with_label = pd.concat([tfidf_df, data['label']], axis=1)\n",
    "top_tfidf_all_labels = tfidf_with_label.drop('label', axis=1).sum().sort_values(ascending=False).head(10)\n",
    "\n",
    "# Filter TF-IDF DataFrame based on label value\n",
    "top_tfidf_label_0 = tfidf_with_label[tfidf_with_label['label'] == 0].sum().sort_values(ascending=False).head(10)\n",
    "\n",
    "# Calculate the sum of TF-IDF scores for each term across documents with label 1\n",
    "top_tfidf_label_1 = tfidf_with_label[tfidf_with_label['label'] == 1].sum().sort_values(ascending=False).head(10)\n",
    "\n",
    "# Print the top TF-IDF values for each label\n",
    "print(\"Top TF-IDF values for label 0:\")\n",
    "print(top_tfidf_label_0)\n",
    "\n",
    "print(\"\\nTop TF-IDF values for label 1:\")\n",
    "print(top_tfidf_label_1.drop('label'))\n",
    "                                   \n",
    "print(\"\\nTop TF-IDF values for all labels:\")\n",
    "print(top_tfidf_all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b37f34a-95c7-4b7b-a014-847d0bf898b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>aahon sa</th>\n",
       "      <th>aba eh</th>\n",
       "      <th>abandoned my</th>\n",
       "      <th>abias cbn</th>\n",
       "      <th>ability to</th>\n",
       "      <th>able to</th>\n",
       "      <th>abnormal says</th>\n",
       "      <th>about and</th>\n",
       "      <th>about being</th>\n",
       "      <th>...</th>\n",
       "      <th>yung tipong</th>\n",
       "      <th>yung tono</th>\n",
       "      <th>yung totoo</th>\n",
       "      <th>yung tv</th>\n",
       "      <th>yung tvc</th>\n",
       "      <th>yung vote</th>\n",
       "      <th>yung votes</th>\n",
       "      <th>yung walang</th>\n",
       "      <th>zero vote</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gastos ni sa political ads halos na inaasahan na ni na may mga taong</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tang ina tuwid na daan daw eh sya nga di straight</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>salamat sa walang sawang suporta ng mga taga makati ang pagbabalik in makati</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>putangina mo takbo pa</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>with selective amnesia forgetting about the past six years he spent preparing to be</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18832</th>\n",
       "      <td>if hoe is talking to your man she is not the problem he is the fact that hoes feel welcomed reflects on how he acts behi</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18833</th>\n",
       "      <td>sounds like bad bitch</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18834</th>\n",
       "      <td>is that all pussy bitch</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18835</th>\n",
       "      <td>seen scooby hoe some niggas at the gardens too</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18836</th>\n",
       "      <td>this bitch just blew my high</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18837 rows × 12804 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                           text  \\\n",
       "0                                                          gastos ni sa political ads halos na inaasahan na ni na may mga taong   \n",
       "1                                                                             tang ina tuwid na daan daw eh sya nga di straight   \n",
       "2                                                  salamat sa walang sawang suporta ng mga taga makati ang pagbabalik in makati   \n",
       "3                                                                                                         putangina mo takbo pa   \n",
       "4                                           with selective amnesia forgetting about the past six years he spent preparing to be   \n",
       "...                                                                                                                         ...   \n",
       "18832  if hoe is talking to your man she is not the problem he is the fact that hoes feel welcomed reflects on how he acts behi   \n",
       "18833                                                                                                     sounds like bad bitch   \n",
       "18834                                                                                                   is that all pussy bitch   \n",
       "18835                                                                            seen scooby hoe some niggas at the gardens too   \n",
       "18836                                                                                              this bitch just blew my high   \n",
       "\n",
       "       aahon sa  aba eh  abandoned my  abias cbn  ability to  able to  \\\n",
       "0           0.0     0.0           0.0        0.0         0.0      0.0   \n",
       "1           0.0     0.0           0.0        0.0         0.0      0.0   \n",
       "2           0.0     0.0           0.0        0.0         0.0      0.0   \n",
       "3           0.0     0.0           0.0        0.0         0.0      0.0   \n",
       "4           0.0     0.0           0.0        0.0         0.0      0.0   \n",
       "...         ...     ...           ...        ...         ...      ...   \n",
       "18832       0.0     0.0           0.0        0.0         0.0      0.0   \n",
       "18833       0.0     0.0           0.0        0.0         0.0      0.0   \n",
       "18834       0.0     0.0           0.0        0.0         0.0      0.0   \n",
       "18835       0.0     0.0           0.0        0.0         0.0      0.0   \n",
       "18836       0.0     0.0           0.0        0.0         0.0      0.0   \n",
       "\n",
       "       abnormal says  about and  about being  ...  yung tipong  yung tono  \\\n",
       "0                0.0        0.0          0.0  ...          0.0        0.0   \n",
       "1                0.0        0.0          0.0  ...          0.0        0.0   \n",
       "2                0.0        0.0          0.0  ...          0.0        0.0   \n",
       "3                0.0        0.0          0.0  ...          0.0        0.0   \n",
       "4                0.0        0.0          0.0  ...          0.0        0.0   \n",
       "...              ...        ...          ...  ...          ...        ...   \n",
       "18832            0.0        0.0          0.0  ...          0.0        0.0   \n",
       "18833            0.0        0.0          0.0  ...          0.0        0.0   \n",
       "18834            0.0        0.0          0.0  ...          0.0        0.0   \n",
       "18835            0.0        0.0          0.0  ...          0.0        0.0   \n",
       "18836            0.0        0.0          0.0  ...          0.0        0.0   \n",
       "\n",
       "       yung totoo  yung tv  yung tvc  yung vote  yung votes  yung walang  \\\n",
       "0             0.0      0.0       0.0        0.0         0.0          0.0   \n",
       "1             0.0      0.0       0.0        0.0         0.0          0.0   \n",
       "2             0.0      0.0       0.0        0.0         0.0          0.0   \n",
       "3             0.0      0.0       0.0        0.0         0.0          0.0   \n",
       "4             0.0      0.0       0.0        0.0         0.0          0.0   \n",
       "...           ...      ...       ...        ...         ...          ...   \n",
       "18832         0.0      0.0       0.0        0.0         0.0          0.0   \n",
       "18833         0.0      0.0       0.0        0.0         0.0          0.0   \n",
       "18834         0.0      0.0       0.0        0.0         0.0          0.0   \n",
       "18835         0.0      0.0       0.0        0.0         0.0          0.0   \n",
       "18836         0.0      0.0       0.0        0.0         0.0          0.0   \n",
       "\n",
       "       zero vote  label  \n",
       "0            0.0      0  \n",
       "1            0.0      1  \n",
       "2            0.0      0  \n",
       "3            0.0      1  \n",
       "4            0.0      0  \n",
       "...          ...    ...  \n",
       "18832        0.0      1  \n",
       "18833        0.0      1  \n",
       "18834        0.0      1  \n",
       "18835        0.0      1  \n",
       "18836        0.0      1  \n",
       "\n",
       "[18837 rows x 12804 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = pd.concat([data['text'], tfidf_df, data['label']], axis=1)\n",
    "result_df.columns = ['text'] + list(tfidf_df.columns) + ['label']\n",
    "\n",
    "\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40a97521-41e3-4440-a3e7-79bf71ed5bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data splitting time: 2.5688 seconds\n",
      "Train: (11207, 12802), Val: (1978, 12802), Test: (5652, 12802)\n"
     ]
    }
   ],
   "source": [
    "# df = result_df.drop('text', axis=1)\n",
    "# X = df.drop('label', axis=1)\n",
    "# y = df['label']\n",
    "seed_value = 5\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "#X_train, y_train for training\n",
    "#X_test, y_test for testing \n",
    "#X_val, y_holdout for \n",
    "\n",
    "start_time = time.perf_counter()\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_df, result_df['label'], test_size=0.3, random_state=seed_value)\n",
    "X_train, X_val, y_train, y_holdout = train_test_split(X_train, y_train, test_size=0.15, random_state=seed_value)\n",
    "\n",
    "elapsed_time = time.perf_counter() - start_time\n",
    "\n",
    "print(f\"Data splitting time: {elapsed_time:.4f} seconds\")\n",
    "print('Train: %s, Val: %s, Test: %s' % (X_train.shape, X_val.shape, X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c5cf6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT fitting time: 13039.78 seconds\n",
      "Decision Tree Classifier - Best Parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10}\n"
     ]
    }
   ],
   "source": [
    "# Model 1: Decision Tree Classifier\n",
    "decision_tree = DecisionTreeClassifier() #initialize dt\n",
    "param_grid_dt = {\n",
    "    'criterion': ['gini'],\n",
    "    'max_depth': [50, 100, None], \n",
    "    'min_samples_split': [5, 10, 20], \n",
    "    'min_samples_leaf': [1, 5, 10]  \n",
    "}\n",
    "\n",
    "dt_cv = GridSearchCV(decision_tree, param_grid_dt, cv=3, n_jobs=n_jobs)\n",
    "\n",
    "\n",
    "# Record start time before fitting\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit the model to training data\n",
    "dt_cv.fit(X_train, y_train)\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "# Print fitting time\n",
    "print(f\"DT fitting time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Access the best model\n",
    "model_1 = dt_cv.best_estimator_\n",
    "\n",
    "# Get feature importances from the best model\n",
    "feature_names = model_1.feature_importances_\n",
    "\n",
    "print(\"Decision Tree Classifier - Best Parameters:\", dt_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec486dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the cross-validation results from GridSearchCV for Decision Tree Classifier\n",
    "results_dt = dt_cv.cv_results_\n",
    "\n",
    "# Extract the mean test scores and parameters\n",
    "mean_test_scores_dt = results_dt['mean_test_score']\n",
    "params_dt = results_dt['params']\n",
    "\n",
    "# Create a list of tuples containing mean test scores and corresponding hyperparameters\n",
    "score_param_pairs_dt = list(zip(mean_test_scores_dt, params_dt))\n",
    "\n",
    "# Sort the list of tuples based on mean test scores in descending order\n",
    "score_param_pairs_dt.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# Select the top N performers (you can adjust N as needed)\n",
    "top_n_dt = 10\n",
    "top_performers_dt = score_param_pairs_dt[:top_n_dt]\n",
    "\n",
    "# Print out the mean test scores for the top performers with line breaks\n",
    "for mean_test_score, params in top_performers_dt:\n",
    "    print(f'Mean test score for hyperparameters {params}:\\n{mean_test_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52ca369c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the cross-validation results from GridSearchCV for Decision Tree Classifier\n",
    "#results_dt = dt_cv.cv_results_\n",
    "\n",
    "## Print out the mean test scores for each fold\n",
    "#for fold in range(dt_cv.cv):\n",
    "#    fold_prefix = f\"Fold {fold + 1}: \"\n",
    "#    fold_test_scores = results_dt[f\"split{fold}_test_score\"]\n",
    "#    print(f\"{fold_prefix}Test scores: {fold_test_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e47d164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total DT prediction time: 1.2645 seconds\n"
     ]
    }
   ],
   "source": [
    "#define timer\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "#prediction\n",
    "holdout_pred_1 = model_1.predict(X_val)\n",
    "test_pred_1 = model_1.predict(X_test)\n",
    "\n",
    "#prediction into dataframe\n",
    "holdout_pred_1 = pd.DataFrame(holdout_pred_1)\n",
    "test_pred_1 = pd.DataFrame(test_pred_1)\n",
    "elapsed_time = time.perf_counter() - start_time\n",
    "print(f\"Total DT prediction time: {elapsed_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f1b978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: K-Nearest Neighbors Classifier\n",
    "knn = KNeighborsClassifier()\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [1, 3, 5, 7, 9],\n",
    "    'weights': ['distance'],\n",
    "    'metric': ['euclidean','manhattan']\n",
    "}\n",
    "knn_cv = GridSearchCV(knn, param_grid_knn, cv=3, n_jobs=n_jobs)\n",
    "\n",
    "# Record start time before fitting\n",
    "start_time = time.time()\n",
    "\n",
    "knn_cv.fit(X_train, y_train)\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "# Print fitting time\n",
    "print(f\"KNN fitting time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "model_2 = knn_cv.best_estimator_\n",
    "\n",
    "\n",
    "print(\"K Neighbors Classifier - Best Parameters:\", knn_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc48175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the cross-validation results from GridSearchCV for K-Nearest Neighbors Classifier\n",
    "results_knn = knn_cv.cv_results_\n",
    "\n",
    "# Extract the mean test scores and parameters\n",
    "mean_test_scores_knn = results_knn['mean_test_score']\n",
    "params_knn = results_knn['params']\n",
    "\n",
    "# Create a list of tuples containing mean test scores and corresponding hyperparameters\n",
    "score_param_pairs_knn = list(zip(mean_test_scores_knn, params_knn))\n",
    "\n",
    "# Sort the list of tuples based on mean test scores in descending order\n",
    "score_param_pairs_knn.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# Select the top N performers (you can adjust N as needed)\n",
    "top_n_knn = 10\n",
    "top_performers_knn = score_param_pairs_knn[:top_n_knn]\n",
    "\n",
    "# Print out the mean test scores for the top performers with line breaks\n",
    "for mean_test_score, params in top_performers_knn:\n",
    "    print(f'Mean test score for hyperparameters {params}:\\n{mean_test_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3afb16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define timer\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "#prediction\n",
    "holdout_pred_2 = model_2.predict(X_val)\n",
    "test_pred_2 = model_2.predict(X_test)\n",
    "\n",
    "#prediction into dataframe\n",
    "holdout_pred_2 = pd.DataFrame(holdout_pred_2)\n",
    "test_pred_2 = pd.DataFrame(test_pred_2)\n",
    "elapsed_time = time.perf_counter() - start_time\n",
    "print(f\"Total KNN prediction time: {elapsed_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0ea224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: Gaussian Naive Bayes Classifier\n",
    "gnb = GaussianNB()\n",
    "param_grid_gnb = {\n",
    "    'var_smoothing': [10, 5, 1, 0.1, 0.01,0.001, 0.00000001, 0.000000001]\n",
    "}\n",
    "gnb_cv = GridSearchCV(gnb, param_grid_gnb, cv=3, n_jobs=n_jobs)\n",
    "\n",
    "# Record start time before fitting\n",
    "start_time = time.time()\n",
    "gnb_cv.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "# Print fitting time\n",
    "print(f\"GNB fitting time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "model_3 = gnb_cv.best_estimator_\n",
    "print(\"GNB Classifier - Best Parameters:\", gnb_cv.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06218171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the cross-validation results from GridSearchCV for Gaussian Naive Bayes Classifier\n",
    "results_gnb = gnb_cv.cv_results_\n",
    "\n",
    "# Extract the mean test scores and parameters\n",
    "mean_test_scores_gnb = results_gnb['mean_test_score']\n",
    "params_gnb = results_gnb['params']\n",
    "\n",
    "# Create a list of tuples containing mean test scores and corresponding hyperparameters\n",
    "score_param_pairs_gnb = list(zip(mean_test_scores_gnb, params_gnb))\n",
    "\n",
    "# Sort the list of tuples based on mean test scores in descending order\n",
    "score_param_pairs_gnb.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# Select the top N performers (you can adjust N as needed)\n",
    "top_n_gnb = 8\n",
    "top_performers_gnb = score_param_pairs_gnb[:top_n_gnb]\n",
    "\n",
    "# Print out the mean test scores for the top performers with line breaks\n",
    "for mean_test_score, params in top_performers_gnb:\n",
    "    print(f'Mean test score for hyperparameters {params}:\\n{mean_test_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405bc2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define timer\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "#prediction\n",
    "holdout_pred_3 = model_3.predict(X_val)\n",
    "test_pred_3 = model_3.predict(X_test)\n",
    "\n",
    "#prediction into dataframe\n",
    "holdout_pred_3 = pd.DataFrame(holdout_pred_3)\n",
    "test_pred_3 = pd.DataFrame(test_pred_3)\n",
    "elapsed_time = time.perf_counter() - start_time\n",
    "print(f\"Total GNB prediction time: {elapsed_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9829e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4: Support Vector Machine\n",
    "svm =SVC()\n",
    "param_grid_svm = {\n",
    "    'C': [0.1, 1, 10, 2],\n",
    "    'kernel': ['linear', 'sigmoid']\n",
    "}\n",
    "svm_cv = GridSearchCV(svm, param_grid_svm, cv=3, n_jobs=n_jobs)\n",
    "\n",
    "# Record start time before fitting\n",
    "start_time = time.time()\n",
    "svm_cv.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "model_4 = svm_cv.best_estimator_\n",
    "# Print fitting time\n",
    "print(f\"SVM fitting time: {elapsed_time:.2f} seconds\")\n",
    "print(\"Support Vector Classifier - Best Parameters:\", svm_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636bc370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the cross-validation results from GridSearchCV for Support Vector Machine Classifier\n",
    "results_svm = svm_cv.cv_results_\n",
    "\n",
    "# Extract the mean test scores and parameters\n",
    "mean_test_scores_svm = results_svm['mean_test_score']\n",
    "params_svm = results_svm['params']\n",
    "\n",
    "# Create a list of tuples containing mean test scores and corresponding hyperparameters\n",
    "score_param_pairs_svm = list(zip(mean_test_scores_svm, params_svm))\n",
    "\n",
    "# Sort the list of tuples based on mean test scores in descending order\n",
    "score_param_pairs_svm.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# Select the top N performers (you can adjust N as needed)\n",
    "top_n_svm = 5\n",
    "top_performers_svm = score_param_pairs_svm[:top_n_svm]\n",
    "\n",
    "# Print out the mean test scores for the top performers with line breaks\n",
    "for mean_test_score, params in top_performers_svm:\n",
    "    print(f'Mean test score for hyperparameters {params}:\\n{mean_test_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81926e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define timer\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "#prediction\n",
    "holdout_pred_4 = model_4.predict(X_val)\n",
    "test_pred_4 = model_4.predict(X_test)\n",
    "\n",
    "#prediction into dataframe\n",
    "holdout_pred_4 = pd.DataFrame(holdout_pred_4)\n",
    "test_pred_4 = pd.DataFrame(test_pred_4)\n",
    "elapsed_time = time.perf_counter() - start_time\n",
    "print(f\"Total SVM prediction time: {elapsed_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fae80a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "685f0b6f-d3c9-4f95-b62e-795faa69d79a",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b4c42c-c66e-46ba-90f3-e3b52d2970e4",
   "metadata": {},
   "source": [
    "# Base Models on Validation/Holdout Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16df6e7-bc83-4ecf-bc92-7dbdf52bb932",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b91c18f-3436-4b3f-aef9-0d9752e2a03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train: %s, Val: %s, Test: %s' % (X_train.shape, X_val.shape, X_test.shape))\n",
    "# Model 1: Decision Tree Classifier\n",
    "accuracy_1 = accuracy_score(y_holdout, holdout_pred_1)\n",
    "precision_1 = precision_score(y_holdout, holdout_pred_1)\n",
    "recall_1 = recall_score(y_holdout, holdout_pred_1)\n",
    "F1Score_1 = f1_score(y_holdout, holdout_pred_1)\n",
    "\n",
    "conf_matrix_1 = confusion_matrix(y_holdout, holdout_pred_1)\n",
    "\n",
    "# Convert scores to percentages\n",
    "accuracy_1_percent = accuracy_1 * 100\n",
    "precision_1_percent = precision_1 * 100\n",
    "recall_1_percent = recall_1 * 100\n",
    "F1Score_1_percent = F1Score_1 * 100\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_1, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
    "plt.xlabel('Overall Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix - Model 1: Decision Tree (DT)')\n",
    "plt.text(0, -0.1, f'Accuracy: {accuracy_1_percent:.2f}%\\nPrecision: {precision_1_percent:.2f}%\\nRecall: {recall_1_percent:.2f}%\\nF1 Score: {F1Score_1_percent:.2f}%', \n",
    "         horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "# Model 2: K-Nearest Neighbors Classifier\n",
    "accuracy_2 = accuracy_score(y_holdout, holdout_pred_2)\n",
    "precision_2 = precision_score(y_holdout, holdout_pred_2)\n",
    "recall_2 = recall_score(y_holdout, holdout_pred_2)\n",
    "F1Score_2 = f1_score(y_holdout, holdout_pred_2)\n",
    "\n",
    "conf_matrix_2 = confusion_matrix(y_holdout, holdout_pred_2)\n",
    "# Convert scores to percentages\n",
    "accuracy_2_percent = accuracy_2 * 100\n",
    "precision_2_percent = precision_2 * 100\n",
    "recall_2_percent = recall_2 * 100\n",
    "F1Score_2_percent = F1Score_2 * 100\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_2, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix - Model 2: K-Nearest Neighbors (KNN)')\n",
    "plt.text(0, -0.1, f'Accuracy: {accuracy_2_percent:.2f}%\\nPrecision: {precision_2_percent:.2f}%\\nRecall: {recall_2_percent:.2f}%\\nF1 Score: {F1Score_2_percent:.2f}%', \n",
    "         horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "plt.show()\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------------#\n",
    "# Model 3: Gaussian Naive Bayes Classifier\n",
    "accuracy_3 = accuracy_score(y_holdout, holdout_pred_3)\n",
    "precision_3 = precision_score(y_holdout, holdout_pred_3)\n",
    "recall_3 = recall_score(y_holdout, holdout_pred_3)\n",
    "F1Score_3 = f1_score(y_holdout, holdout_pred_3)\n",
    "\n",
    "conf_matrix_3 = confusion_matrix(y_holdout, holdout_pred_3)\n",
    "\n",
    "# Convert scores to percentages\n",
    "accuracy_3_percent = accuracy_3 * 100\n",
    "precision_3_percent = precision_3 * 100\n",
    "recall_3_percent = recall_3 * 100\n",
    "F1Score_3_percent = F1Score_3 * 100\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_3, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix - Model 3: Gaussian Naive Bayes (GNB)')\n",
    "plt.text(0, -0.1, f'Accuracy: {accuracy_3_percent:.2f}%\\nPrecision: {precision_3_percent:.2f}%\\nRecall: {recall_3_percent:.2f}%\\nF1 Score: {F1Score_3_percent:.2f}%', \n",
    "         horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "plt.show()\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "# Model 4: Support Vector Machine (SVM) Classifier\n",
    "accuracy_4 = accuracy_score(y_holdout, holdout_pred_4)\n",
    "precision_4 = precision_score(y_holdout, holdout_pred_4)\n",
    "recall_4 = recall_score(y_holdout, holdout_pred_4)\n",
    "F1Score_4 = f1_score(y_holdout, holdout_pred_4)\n",
    "conf_matrix_4 = confusion_matrix(y_holdout, holdout_pred_4)\n",
    "\n",
    "# Convert scores to percentages\n",
    "accuracy_4_percent = accuracy_4 * 100\n",
    "precision_4_percent = precision_4 * 100\n",
    "recall_4_percent = recall_4 * 100\n",
    "F1Score_4_percent = F1Score_4 * 100\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_4, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix - Model 4: SVM')\n",
    "plt.text(0, -0.1, f'Accuracy: {accuracy_4_percent:.2f}%\\nPrecision: {precision_4_percent:.2f}%\\nRecall: {recall_4_percent:.2f}%\\nF1 Score: {F1Score_4_percent:.2f}%', \n",
    "         horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "plt.show()\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fa201b-9403-4785-a12e-ee80a94a7bcc",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e91226-0b79-4478-af55-1f3c4b40ff64",
   "metadata": {},
   "source": [
    "# Metamodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13413551-ab97-4ee2-a904-51e2edca9b9a",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e299ed-ab00-4ede-bdcc-e1a59481440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.reset_index(drop=True, inplace=True)\n",
    "pd.DataFrame(holdout_pred_1).reset_index(drop=True, inplace=True) #DT\n",
    "pd.DataFrame(holdout_pred_2).reset_index(drop=True, inplace=True) #KNN\n",
    "pd.DataFrame(holdout_pred_3).reset_index(drop=True, inplace=True) #GNB\n",
    "pd.DataFrame(holdout_pred_4).reset_index(drop=True, inplace=True) #SVM\n",
    "\n",
    "X_blend_val = pd.concat([X_val, pd.DataFrame(holdout_pred_1),  pd.DataFrame(holdout_pred_3), pd.DataFrame(holdout_pred_4)], axis=1)\n",
    "#X_blend_val = pd.concat([X_val ,  pd.DataFrame(holdout_pred_1), pd.DataFrame(holdout_pred_4)], axis=1)\n",
    "\n",
    "\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "pd.DataFrame(test_pred_1).reset_index(drop=True, inplace=True)\n",
    "pd.DataFrame(test_pred_2).reset_index(drop=True, inplace=True)\n",
    "pd.DataFrame(test_pred_3).reset_index(drop=True, inplace=True)\n",
    "pd.DataFrame(test_pred_4).reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_blend_test = pd.concat([X_test, pd.DataFrame(test_pred_1), pd.DataFrame(test_pred_3), pd.DataFrame(test_pred_4)], axis=1)\n",
    "#X_blend_test = pd.concat([X_test,  pd.DataFrame(test_pred_1), pd.DataFrame(test_pred_4)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0250ca77-5f97-4345-92e3-f53fc881eb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#same ^^^^^ \n",
    "#, holdout_pred_5\n",
    "#test_pred_5\n",
    "#X_blend_val = pd.concat([ holdout_pred_3, holdout_pred_4], axis=1)\n",
    "#X_blend_val = pd.concat([holdout_pred_1, holdout_pred_2, holdout_pred_3, holdout_pred_4], axis=1)\n",
    "#X_blend_test = pd.concat([ test_pred_3, test_pred_4], axis=1)\n",
    "#X_blend_test = pd.concat([test_pred_1, test_pred_2, test_pred_3, test_pred_4], axis=1)\n",
    "X_blend_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ce6445-33e2-485d-bc14-377725dfb9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR metamodel\n",
    "lr = LogisticRegression()\n",
    "param_grid_lr = {\n",
    "    'max_iter': [100,500],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'sag', 'saga'],#'newton-cg',\n",
    "    'C': [1, 10,0.1, 0.001, 0.0001],\n",
    "}\n",
    "\n",
    "# For Logistic Regression\n",
    "lr_cv = GridSearchCV(lr, param_grid_lr, cv=3, n_jobs=n_jobs, scoring='accuracy')\n",
    "\n",
    "\n",
    "# Record start time before fitting\n",
    "start_time = time.time()\n",
    "lr_cv.fit(X_blend_val, y_holdout)\n",
    "\n",
    "meta_model = lr_cv.best_estimator_\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "# Print fitting time\n",
    "print(f\"LR fitting time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "print(\"Logistic Regression - Best Parameters:\", lr_cv.best_params_)\n",
    "# Fit the meta-model on the validation predictions\n",
    "#meta_model.fit(X_blend_val, y_holdout)\n",
    "\n",
    "# Make predictions using the meta-model on the test predictions\n",
    "blend_pred = meta_model.predict(X_blend_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514f415f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_blend = confusion_matrix(y_test, blend_pred)\n",
    "blend_accuracy = accuracy_score(y_test, blend_pred)\n",
    "blend_precision = precision_score(y_test, blend_pred)\n",
    "blend_recall = recall_score(y_test, blend_pred)\n",
    "blend_f1 = f1_score(y_test, blend_pred)\n",
    "\n",
    "# Calculate percentages\n",
    "blend_accuracy_percent = blend_accuracy * 100\n",
    "blend_precision_percent = blend_precision * 100\n",
    "blend_recall_percent = blend_recall * 100\n",
    "blend_f1_percent = blend_f1 * 100\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_blend, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix - Meta Model: LR')\n",
    "plt.text(0, -0.1, f'Accuracy: {blend_accuracy_percent:.2f}%\\nPrecision: {blend_precision_percent:.2f}%\\nRecall: {blend_recall_percent:.2f}%\\nF1 Score: {blend_f1_percent:.2f}%', \n",
    "         horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cebe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the cross-validation results from GridSearchCV for Logistic Regression\n",
    "results_lr = lr_cv.cv_results_\n",
    "\n",
    "# Extract the mean test scores and parameters\n",
    "mean_test_scores_lr = results_lr['mean_test_score']\n",
    "params_lr = results_lr['params']\n",
    "\n",
    "# Combine mean test scores and parameters into tuples\n",
    "score_param_pairs = list(zip(mean_test_scores_lr, params_lr))\n",
    "\n",
    "# Sort the list of tuples based on mean test scores in descending order\n",
    "score_param_pairs_sorted = sorted(score_param_pairs, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# Select the top N performers (you can adjust N as needed)\n",
    "top_performers = score_param_pairs_sorted[:15]\n",
    "\n",
    "# Print out the top performers\n",
    "for mean_test_score, params in top_performers:\n",
    "    print(f'Mean test score for hyperparameters {params}:\\n{mean_test_score}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86ba505-db32-44c8-9e2d-b854da87367e",
   "metadata": {},
   "source": [
    "# Base Models Classification on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fccdea-d0b2-4a78-9af3-e7d5ccc29217",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DT: Test Set Results\n",
    "dt_accuracy = accuracy_score(y_test, test_pred_1)\n",
    "dt_precision = precision_score(y_test, test_pred_1)\n",
    "dt_recall = recall_score(y_test, test_pred_1)\n",
    "dt_f1 = f1_score(y_test, test_pred_1)\n",
    "\n",
    "conf_matrix_dt_test = confusion_matrix(y_test, test_pred_1)\n",
    "\n",
    "# Calculate percentages\n",
    "dt_accuracy_percent = dt_accuracy * 100\n",
    "dt_precision_percent = dt_precision * 100\n",
    "dt_recall_percent = dt_recall * 100\n",
    "dt_f1_percent = dt_f1 * 100\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_dt_test, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix - Decision Tree (Test Set)')\n",
    "plt.text(0, -0.1, f'Accuracy: {dt_accuracy_percent:.2f}%\\nPrecision: {dt_precision_percent:.2f}%\\nRecall: {dt_recall_percent:.2f}%\\nF1 Score: {dt_f1_percent:.2f}%', \n",
    "         horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "plt.show()\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------------#\n",
    "#KNN: Test Set Results\n",
    "knn_accuracy = accuracy_score(y_test, test_pred_2)\n",
    "knn_precision = precision_score(y_test, test_pred_2)\n",
    "knn_recall = recall_score(y_test, test_pred_2)\n",
    "knn_f1 = f1_score(y_test, test_pred_2)\n",
    "\n",
    "conf_matrix_knn_test = confusion_matrix(y_test, test_pred_2)\n",
    "\n",
    "# Calculate percentages\n",
    "knn_accuracy_percent = knn_accuracy * 100\n",
    "knn_precision_percent = knn_precision * 100\n",
    "knn_recall_percent = knn_recall * 100\n",
    "knn_f1_percent = knn_f1 * 100\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_knn_test, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix - KNN (Test Set)')\n",
    "plt.text(0, -0.1, f'Accuracy: {knn_accuracy_percent:.2f}%\\nPrecision: {knn_precision_percent:.2f}%\\nRecall: {knn_recall_percent:.2f}%\\nF1 Score: {knn_f1_percent:.2f}%', \n",
    "         horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "plt.show()\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------------------#\n",
    "#GNB: Test Set Results\n",
    "gnb_accuracy = accuracy_score(y_test, test_pred_3)\n",
    "gnb_precision = precision_score(y_test, test_pred_3)\n",
    "gnb_recall = recall_score(y_test, test_pred_3)\n",
    "gnb_f1 = f1_score(y_test, test_pred_3)\n",
    "\n",
    "conf_matrix_gnb_test = confusion_matrix(y_test, test_pred_3)\n",
    "\n",
    "# Calculate percentages\n",
    "gnb_accuracy_percent = gnb_accuracy * 100\n",
    "gnb_precision_percent = gnb_precision * 100\n",
    "gnb_recall_percent = gnb_recall * 100\n",
    "gnb_f1_percent = gnb_f1 * 100\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_gnb_test, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix - GNB (Test Set)')\n",
    "plt.text(0, -0.1, f'Accuracy: {gnb_accuracy_percent:.2f}%\\nPrecision: {gnb_precision_percent:.2f}%\\nRecall: {gnb_recall_percent:.2f}%\\nF1 Score: {gnb_f1_percent:.2f}%', \n",
    "         horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "plt.show()\n",
    "#------------------------------------------------------------------------------------------------------------------------------------#\n",
    "#SVM: Test Set Results\n",
    "svm_accuracy = accuracy_score(y_test, test_pred_4)\n",
    "svm_precision = precision_score(y_test, test_pred_4)\n",
    "svm_recall = recall_score(y_test, test_pred_4)\n",
    "svm_f1 = f1_score(y_test, test_pred_4)\n",
    "\n",
    "conf_matrix_svm_test = confusion_matrix(y_test, test_pred_4)\n",
    "\n",
    "# Calculate percentages\n",
    "svm_accuracy_percent = svm_accuracy * 100\n",
    "svm_precision_percent = svm_precision * 100\n",
    "svm_recall_percent = svm_recall * 100\n",
    "svm_f1_percent = svm_f1 * 100\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_svm_test, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix - SVM (Test Set)')\n",
    "plt.text(0, -0.1, f'Accuracy: {svm_accuracy_percent:.2f}%\\nPrecision: {svm_precision_percent:.2f}%\\nRecall: {svm_recall_percent:.2f}%\\nF1 Score: {svm_f1_percent:.2f}%', \n",
    "         horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "plt.show()\n",
    "#conf_matrix_svm_test = confusion_matrix(y_test, test_pred_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39591060",
   "metadata": {},
   "source": [
    "# Holdout Prediction Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be42ddf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_holdout\n",
    "y_holdout_df = y_holdout.reset_index(name='true label')\n",
    "holdout_pred_1_df = pd.DataFrame(holdout_pred_1)\n",
    "holdout_pred_2_df = pd.DataFrame(holdout_pred_2)\n",
    "holdout_pred_3_df = pd.DataFrame(holdout_pred_3)\n",
    "holdout_pred_4_df = pd.DataFrame(holdout_pred_4)\n",
    "\n",
    "combined_holdout_df = pd.concat([y_holdout_df, holdout_pred_1_df.rename(columns={0: 'dt prediction'}), holdout_pred_2_df.rename(columns={0: 'knn prediction'}), holdout_pred_3_df.rename(columns={0: 'gnb prediction'}), holdout_pred_4_df.rename(columns={0: 'svm prediction'})], axis=1)\n",
    "combined_holdout_df.to_csv('holdout_predictions.csv')\n",
    "combined_holdout_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccfb56e",
   "metadata": {},
   "source": [
    "# Test Set Prediction Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfbc5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_df = y_test.reset_index(name='true label')\n",
    "\n",
    "test_pred_1_df = pd.DataFrame(test_pred_1)\n",
    "test_pred_2_df = pd.DataFrame(test_pred_2)\n",
    "test_pred_3_df = pd.DataFrame(test_pred_3)\n",
    "test_pred_4_df = pd.DataFrame(test_pred_4)\n",
    "blend_pred_df = pd.DataFrame(blend_pred)\n",
    "\n",
    "combined_test_df = pd.concat([y_test_df, test_pred_1_df.rename(columns={0: 'dt prediction'}), test_pred_2_df.rename(columns={0: 'knn prediction'}), test_pred_3_df.rename(columns={0: 'gnb prediction'}), test_pred_4_df.rename(columns={0: 'svm prediction'}), blend_pred_df.rename(columns={0: 'lr prediction'}), ], axis=1)\n",
    "combined_test_df.to_csv('test_predictions.csv')\n",
    "combined_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ca7570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PANG EXPORT TRAINED MODEL (DONT DELETE)\n",
    "\n",
    "\n",
    " joblib.dump(model_1, \"DT.pkl\")\n",
    " joblib.dump(model_2, \"KNN.pkl\")\n",
    " joblib.dump(model_3, \"GNB.pkl\")\n",
    " joblib.dump(model_4, \"SVM.pkl\")\n",
    " joblib.dump(meta_model, \"LR-metamodel.pkl\")\n",
    " joblib.dump(tfidf_vectorizer, \"tfidf_vectorizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eb364a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ebdb1acd-1d84-4174-9f48-b2cbdf4a5edf",
   "metadata": {},
   "source": [
    "*** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619baeda-9b8e-45e7-a545-fd8e565ee5a8",
   "metadata": {},
   "source": [
    "# GUI Reminder: add the functionalities such as preprocess, word char, word minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63f8351-40b9-46d1-aff4-d205e6be48dc",
   "metadata": {},
   "source": [
    "*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce88f46-0a47-4305-be12-16190b3d6c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERy latest with base model result\n",
    "import tkinter as tk\n",
    "from tkinter import Text, Button, Label, messagebox\n",
    "import pandas as pd\n",
    "\n",
    "def classify_text():\n",
    "    input_text = text_input.get(\"1.0\", \"end-1c\")\n",
    "    if input_text:\n",
    "        # TF-IDF Vectorization for the input text\n",
    "        input_tfidf = tfidf_vectorizer.transform([input_text])\n",
    "        input_df = pd.DataFrame(input_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names())\n",
    "        \n",
    "        # Make predictions using base models\n",
    "        input_pred_1 = model_1.predict(input_df)\n",
    "        input_pred_2 = model_2.predict(input_df)\n",
    "        input_pred_3 = model_3.predict(input_df)\n",
    "        input_pred_4 = model_4.predict(input_df)\n",
    "        \n",
    "        # Display predictions of base models\n",
    "        base_model_results = f\"DT: {'Hate' if input_pred_1[0] else 'Not Hate'}\\n\" + \\\n",
    "                             f\"KNN: {'Hate' if input_pred_2[0] else 'Not Hate'}\\n\" + \\\n",
    "                             f\"GNB: {'Hate' if input_pred_3[0] else 'Not Hate'}\\n\" + \\\n",
    "                             f\"SVM: {'Hate' if input_pred_4[0] else 'Not Hate'}\\n\"\n",
    "        result_label.config(text=base_model_results)\n",
    "        \n",
    "        # Create blended features for input text\n",
    "        #preprocessed into tf-idf vector\n",
    "        input_blend = pd.DataFrame({'Model 1': input_pred_1, 'Model 2': input_pred_2,\n",
    "                                    'Model 3': input_pred_3, 'Model 4': input_pred_4})\n",
    "        \n",
    "        # Make predictions using the meta-model\n",
    "        meta_pred = meta_model.predict(input_blend.values)\n",
    "        \n",
    "        # Display final classification result\n",
    "        final_result = f\"Ensemble result : {'Hate' if meta_pred[0] else 'Not Hate'}\"\n",
    "        result_label.config(text=result_label.cget(\"text\") + \"\\n\\n\" + final_result)\n",
    "        \n",
    "    else:\n",
    "        messagebox.showwarning(\"Warning\", \"Please enter text for analysis\")\n",
    "\n",
    "# Your models and tfidf_vectorizer should be defined before this code\n",
    "\n",
    "# Create GUI window\n",
    "root = tk.Tk()\n",
    "root.title(\"Hate Classification GUI\")\n",
    "\n",
    "# Text Input\n",
    "text_input_label = Label(root, text=\"Enter Text:\")\n",
    "text_input_label.pack()\n",
    "text_input = Text(root, height=30, width=40)\n",
    "text_input.pack()\n",
    "\n",
    "# Analyze Button\n",
    "analyze_button = Button(root, text=\"Analyze\", command=classify_text)\n",
    "analyze_button.pack()\n",
    "\n",
    "# Result Label\n",
    "result_label = Label(root, text=\"Base Model Predictions:\")\n",
    "result_label.pack()\n",
    "\n",
    "# Run the GUI\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d0b38b-950a-40e1-85a1-a9dae68ea180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #old gui with base model result\n",
    "\n",
    "# def classify_text():\n",
    "#     input_text = text_input.get(\"1.0\", \"end-1c\")\n",
    "#     if input_text:\n",
    "#         # TF-IDF Vectorization for the input text\n",
    "\n",
    "       \n",
    "#         input_tfidf = tfidf_vectorizer.transform([input_text])\n",
    "#         input_df = pd.DataFrame(input_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "#         # Make predictions using base models\n",
    "\n",
    "#         input_pred_1 = model_1.predict(input_df)\n",
    "#         input_pred_2 = model_2.predict(input_df)\n",
    "#         input_pred_3 = model_3.predict(input_df)\n",
    "#         input_pred_4 = model_4.predict(input_df)\n",
    "        \n",
    "\n",
    "#         # Create blended features for input text\n",
    "#         input_blend = pd.DataFrame({'Model 1': input_pred_1, 'Model 2': input_pred_2,\n",
    "#                                     'Model 3': input_pred_3, 'Model 4': input_pred_4,})\n",
    "\n",
    "#         # Make predictions using the meta-model\n",
    "#         meta_pred = meta_model.predict(input_blend.values)\n",
    "#         #probability_hate = meta_model.predict_proba(input_blend)[:, 1][0]\n",
    "\n",
    "#         # Display result and probability\n",
    "#         result_label.config(text=f\"Classification Result: {'Hate' if meta_pred[0] else 'Not Hate'}\")\n",
    "        \n",
    "#         #probability_label.config(text=f\"Probability of Hate: {probability_hate:.2%}\")\n",
    "#     else:\n",
    "#         messagebox.showwarning(\"Warning\", \"Please enter text for analysis\")\n",
    "\n",
    "# # Create GUI window\n",
    "# root = tk.Tk()\n",
    "# root.title(\"Hate Classification GUI\")\n",
    "\n",
    "# # Text Input\n",
    "# text_input_label = Label(root, text=\"Enter Text:\")\n",
    "# text_input_label.pack()\n",
    "# text_input = Text(root, height=5, width=40)\n",
    "# text_input.pack()\n",
    "\n",
    "# # Analyze Button\n",
    "# analyze_button = Button(root, text=\"Analyze\", command=classify_text)\n",
    "# analyze_button.pack()\n",
    "\n",
    "# # Result Label\n",
    "# result_label = Label(root, text=\"Classification Result: \")\n",
    "# result_label.pack()\n",
    "\n",
    "# # Probability Label\n",
    "# # probability_label = Label(root, text=\"Probability of Hate: \")\n",
    "# # probability_label.pack()\n",
    "\n",
    "# # Run the GUI\n",
    "# root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962955b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just random shit experii\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "## For DecisionTreeClassifier\n",
    "#param_grid_dt = {\n",
    "#    'criterion': ['gini', 'entropy'],\n",
    "#    'max_depth': [None, 5, 10, 15, 20],\n",
    "#    'min_samples_split': [2, 5, 10],\n",
    "#    'min_samples_leaf': [1, 2, 4]\n",
    "#}\n",
    "\n",
    "## For KNeighborsClassifier\n",
    "#param_grid_knn = {\n",
    "#    'n_neighbors': [3, 5, 7, 9],\n",
    "#    'weights': ['uniform', 'distance'],\n",
    "#    'metric': ['euclidean', 'manhattan']\n",
    "#}\n",
    "\n",
    "## For SVC\n",
    "#param_grid_svc = {\n",
    "#    'C': [0.1, 1, 10, 100],\n",
    "#    'kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "#}\n",
    "\n",
    "## For DecisionTreeClassifier\n",
    "#grid_search_dt = GridSearchCV(model_1, param_grid_dt, cv=5)\n",
    "#grid_search_dt.fit(X_train, y_train)\n",
    "\n",
    "## For KNeighborsClassifier\n",
    "#grid_search_knn = GridSearchCV(model_2, param_grid_knn, cv=5)\n",
    "#grid_search_knn.fit(X_train, y_train)\n",
    "\n",
    "## For SVC\n",
    "#grid_search_svc = GridSearchCV(model_4, param_grid_svc, cv=5)\n",
    "#grid_search_svc.fit(X_train, y_train)\n",
    "\n",
    "#best_dt = grid_search_dt.best_estimator_\n",
    "#best_knn = grid_search_knn.best_estimator_\n",
    "#best_svc = grid_search_svc.best_estimator_\n",
    "\n",
    "#print(\"Decision Tree Classifier - Best Parameters:\", grid_search_dt.best_params_)\n",
    "#print(\"K Neighbors Classifier - Best Parameters:\", grid_search_knn.best_params_)\n",
    "#print(\"Support Vector Classifier - Best Parameters:\", grid_search_svc.best_params_)\n",
    "\n",
    "## Evaluate best models\n",
    "#accuracy_dt = best_dt.score(X_test, y_test)\n",
    "#accuracy_knn = best_knn.score(X_test, y_test)\n",
    "#accuracy_svc = best_svc.score(X_test, y_test)\n",
    "\n",
    "#print(\"Decision Tree Classifier - Test Accuracy:\", accuracy_dt)\n",
    "#print(\"K Neighbors Classifier - Test Accuracy:\", accuracy_knn)\n",
    "#print(\"Support Vector Classifier - Test Accuracy:\", accuracy_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8391c449-ad27-4ace-af75-f104f44f7b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7da5d5-875f-49ca-a776-42725226fd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONT MIND !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "# import tkinter as tk\n",
    "# from tkinter import Text, Button, Label, messagebox\n",
    "# import pandas as pd\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# # Download stopwords data\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# # Define English stopwords\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# def classify_text():\n",
    "#     input_text = text_input.get(\"1.0\", \"end-1c\")\n",
    "#     if input_text:\n",
    "#         # Tokenize input text\n",
    "#         word_tokens = word_tokenize(input_text.lower())\n",
    "        \n",
    "#         # Remove stopwords from input text\n",
    "#         filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "\n",
    "#         # Join filtered words back into a string\n",
    "#         filtered_text = ' '.join(filtered_text)\n",
    "        \n",
    "#         # TF-IDF Vectorization for the filtered text\n",
    "#         input_tfidf = tfidf_vectorizer.transform([filtered_text])\n",
    "#         input_df = pd.DataFrame(input_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "        \n",
    "#         # Make predictions using base models\n",
    "#         input_pred_1 = model_1.predict(input_df)\n",
    "#         input_pred_2 = model_2.predict(input_df)\n",
    "#         input_pred_3 = model_3.predict(input_df)\n",
    "#         input_pred_4 = model_4.predict(input_df)\n",
    "        \n",
    "#         # Display predictions of base models\n",
    "#         base_model_results = f\"DT: {'Hate' if input_pred_1[0] else 'Not Hate'}\\n\" + \\\n",
    "#                              f\"KNN: {'Hate' if input_pred_2[0] else 'Not Hate'}\\n\" + \\\n",
    "#                              f\"GNB: {'Hate' if input_pred_3[0] else 'Not Hate'}\\n\" + \\\n",
    "#                              f\"SVM: {'Hate' if input_pred_4[0] else 'Not Hate'}\\n\"\n",
    "#         result_label.config(text=base_model_results)\n",
    "        \n",
    "#         # Create blended features for input text\n",
    "#         input_blend = pd.DataFrame({'Model 1': input_pred_1, 'Model 2': input_pred_2,\n",
    "#                                     'Model 3': input_pred_3, 'Model 4': input_pred_4})\n",
    "        \n",
    "#         # Make predictions using the meta-model\n",
    "#         meta_pred = meta_model.predict(input_blend.values)\n",
    "        \n",
    "#         # Display final classification result\n",
    "#         final_result = f\"Ensemble result : {'Hate' if meta_pred[0] else 'Not Hate'}\"\n",
    "#         result_label.config(text=result_label.cget(\"text\") + \"\\n\\n\" + final_result)\n",
    "        \n",
    "#     else:\n",
    "#         messagebox.showwarning(\"Warning\", \"Please enter text for analysis\")\n",
    "\n",
    "# # Your models and tfidf_vectorizer should be defined before this code\n",
    "\n",
    "# # Create GUI window\n",
    "# root = tk.Tk()\n",
    "# root.title(\"Hate Classification GUI\")\n",
    "\n",
    "# # Text Input\n",
    "# text_input_label = Label(root, text=\"Enter Text:\")\n",
    "# text_input_label.pack()\n",
    "# text_input = Text(root, height=5, width=40)\n",
    "# text_input.pack()\n",
    "\n",
    "# # Analyze Button\n",
    "# analyze_button = Button(root, text=\"Analyze\", command=classify_text)\n",
    "# analyze_button.pack()\n",
    "\n",
    "# # Result Label\n",
    "# result_label = Label(root, text=\"Base Model Predictions:\")\n",
    "# result_label.pack()\n",
    "\n",
    "# # Run the GUI\n",
    "# root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e91e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def classify_text():\n",
    "    input_text = text_input.get(\"1.0\", \"end-1c\")\n",
    "    if input_text:\n",
    "        # Preprocess the input text\n",
    "        preprocessed_text = lowercasing(input_text)\n",
    "        preprocessed_text = data_deidentification(preprocessed_text)\n",
    "        preprocessed_text = remove_hashtags(preprocessed_text)\n",
    "        preprocessed_text = remove_urls(preprocessed_text)\n",
    "        preprocessed_text = remove_numbers(preprocessed_text)\n",
    "        preprocessed_text = remove_extra_spaces(preprocessed_text)\n",
    "        preprocessed_text = contraction_expansion(preprocessed_text)\n",
    "        preprocessed_text = punctuations_and_abbreviations(preprocessed_text)\n",
    "        preprocessed_text = remove_candidate_name(preprocessed_text)\n",
    "        preprocessed_text = remove_rt_and_single_char(preprocessed_text)\n",
    "        \n",
    "        if not preprocessed_text.strip():\n",
    "            messagebox.showwarning(\"Warning\", \"Please enter text for analysis\")\n",
    "            return\n",
    "        \n",
    "        # TF-IDF Vectorization for the preprocessed text\n",
    "        input_tfidf = tfidf_vectorizer.transform([preprocessed_text])\n",
    "        input_df = pd.DataFrame(input_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "        \n",
    "        # Make predictions using base models\n",
    "        input_pred_1 = model_1.predict(input_df)\n",
    "        input_pred_2 = model_2.predict(input_df)\n",
    "        input_pred_3 = model_3.predict(input_df)\n",
    "        input_pred_4 = model_4.predict(input_df)\n",
    "        \n",
    "        # Display predictions of base models\n",
    "        base_model_results = f\"DT: {'Hate' if input_pred_1[0] else 'Not Hate'}\\n\" + \\\n",
    "                             f\"KNN: {'Hate' if input_pred_2[0] else 'Not Hate'}\\n\" + \\\n",
    "                             f\"GNB: {'Hate' if input_pred_3[0] else 'Not Hate'}\\n\" + \\\n",
    "                             f\"SVM: {'Hate' if input_pred_4[0] else 'Not Hate'}\\n\"\n",
    "        result_label.config(text=base_model_results)\n",
    "        \n",
    "        # Create blended features for input text\n",
    "        input_blend = pd.DataFrame({'Model 1': input_pred_1, 'Model 2': input_pred_2,\n",
    "                                    'Model 3': input_pred_3, 'Model 4': input_pred_4})\n",
    "        \n",
    "        # Make predictions using the meta-model\n",
    "        meta_pred = meta_model.predict(input_blend.values)\n",
    "        \n",
    "        # Display final classification result\n",
    "        final_result = f\"Ensemble result : {'Hate' if meta_pred[0] else 'Not Hate'}\"\n",
    "        result_label.config(text=result_label.cget(\"text\") + \"\\n\\n\" + final_result)\n",
    "        \n",
    "    else:\n",
    "        messagebox.showwarning(\"Warning\", \"Please enter text for analysis\")\n",
    "\n",
    "\n",
    "# Create GUI window\n",
    "root = tk.Tk()\n",
    "root.title(\"Hate Classification GUI\")\n",
    "\n",
    "# Text Input\n",
    "text_input_label = Label(root, text=\"Enter Text:\")\n",
    "text_input_label.pack()\n",
    "text_input = Text(root, height=5, width=40)\n",
    "text_input.pack()\n",
    "\n",
    "# Analyze Button\n",
    "analyze_button = Button(root, text=\"Analyze\", command=classify_text)\n",
    "analyze_button.pack()\n",
    "\n",
    "# Result Label\n",
    "result_label = Label(root, text=\"Base Model Predictions:\")\n",
    "result_label.pack()\n",
    "\n",
    "# Run the GUI\n",
    "root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvthesis",
   "language": "python",
   "name": "venvthesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
